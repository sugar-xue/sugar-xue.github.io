<!DOCTYPE html>


<html lang="zh-CN">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     Sugar
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

</html>

<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover6.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Sugar</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['小猫在午睡，地球在转圈', '心有猛虎，细嗅蔷薇', '知足且上进，温柔且坚定'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a target="_blank" rel="noopener" href="https://curl.qcloud.com/ezctH6FU">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/ten_1.jpg" width="300" alt="腾讯云限时秒杀">
            </a>
        </li>
    
        <li>
            <a target="_blank" rel="noopener" href="https://curl.qcloud.com/kvO7hb43">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/ten_2.jpg" width="300" alt="云服务器全球购低至2折">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-17decentralized"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/01/30/17decentralized/"
    >Decentralized Collaborative Learning of Personalized Models over Networks</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2021/01/30/17decentralized/" class="article-date">
  <time datetime="2021-01-30T13:43:19.000Z" itemprop="datePublished">2021-01-30</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/paper/">paper</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  
考虑点对点的协作网络。本文解决的问题是：每个节点如何与具有相似目标的其他节点进行通信来改善本地模型？作者介绍了两种完全去中心化的算法，一种是受标签传播的启发，旨在平滑预先训练好的局部模型；第二种方法，节点基于本地数据核相邻节点进行迭代更新来共同学习和传播。

# Introduction

数据不断产生，当前从数据中提取信息的主要方式是收集所有用户的个人数据于一个服务器上，然后进行数据挖掘。但是，中心化的方式存在一些问题，比如说一些用户拒绝提供个人数据，带宽和设备花费问题。即使一些算法允许数据分布在用户设备上，通常需要中心端来进行聚合和协调。

在本文中，作者考虑完全去中心化的点对点网络。不同于那些求解全局模型的算法，本文关注于每个节点可以根据自身目标函数学习一个个性化模型。作者假设网络结构已知，该网络结构能够反映出不同节点的相似度（如果两个节点具有相似的目标函数，那么这两个节点在网络中是邻居），每个节点只知道与其直接相邻的节点。一个节点不仅可以根据自身数据学习模型，还可以结合它的邻居。假设每个节点只知道相邻节点的信息，不知道整个网络结构。

作者提出两个算法。第一个是 model propagation：首先，每个节点先基于自己的局部数据学习到模型参数，然后，结合整个网络结构，平滑这些参数。第二个是 collaborative learning，这个算法更加灵活，它通过优化一个模型参数正则化（平滑）和局部模型准确性上的折中问题。作者基于分布式的ADMM算法提出一个异步gossip算法。

# Preliminaries

## Notations and Problem Setting

考虑 $n$ 个节点 $V = [n] = \{1,...,n\}$。凸的损失函数 $l: \mathbb{R}^{p} \times \mathcal{X} \times \mathcal{Y}$，节点 $i$ 的目标是学习模型参数 $\theta_{i} \in \mathbb{R}^{p}$，使得关于未知分布 $\mu_{i}$ 的期望损失 $E_{(x_{i}, y_{i})\sim \mu_{i}}l(\theta_{i}; x_{i}, y_{i})$ 很小。节点 $i$ 具有 $m_{i}$ 个来自分布 $\mu_{i}$ 的 i.i.d 的训练样本 $S_{i} = \{(x_{i}^{j}, y_{i}^{j})\}_{j=1}^{m_{i}}$。允许不同节点的样本量相差很大。每个节点可以最小化局部损失函数得到 $\theta_{i}^{sol}$:

$$
\theta_{i}^{sol} \in \argmin_{\theta \in \mathbb{R}^{p}} L_{i}(\theta) = \sum_{j=1}^{m_{i}} l(\theta;x_{i}^{j}, y_{i}^{j}).
$$

我们目标是通过结合其他节点信息，进一步改善上述模型。考虑一个加权网络结构 $G = (V, E)$，具有 $V$ 个节点，$E \subseteq V \times V$ 为无向边。定义 $W \in \mathbb{R}^{n \times n}$ 为由 $G$ 得到的对称非负加权矩阵，如果 $(i,j) \ne E$ or $i = j$， $W_{ij} = 0$。本文假设权重矩阵已知。定义对角阵 $D\in \mathbb{R}^{n \times n}$，$D_{ii} = \sum_{j=1}^{n} W_{ij}$。节点 $i$ 的邻域 ：$\mathcal{N}_{i} = \{j \ne i: W_{ij} > 0\}$。

# Model Propagation

假设每个节点通过最小化局部损失函数得到各自的模型 $\theta_{i}^{sol}$。由于每个节点上的模型都是在不同大小数据集上考虑得到，作者使用 $c_{i} \in (0,1]$ 定义每个节点模型的可信度。 $c_{i}$ 的值应该和节点 $i$ 的样本量大小呈正相关，可以设置为 $c_{i} = \frac{m_{i}}{\max_{j} m_{j}}$。如果 $m_{i}=0$，可以设置为一个小量。

定义 $\Theta = [\theta_{1}; \theta_{2};...;\theta_{n}] \in \mathbb{R}^{n \times p}$，我们要优化的目标函数为：

![1612018569](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018569.jpg)

第一项二次函数用来平滑相邻节点的参数，当两个节点间权重越大时，节点间参数越相近；第二项的目的是使具有较高置信度的模型的参数不要太远离各自模型上的参数。具有较低置信度的模型的参数被允许具有较大的偏差，容易被相邻节点影响。$D_{ii}$ 的目的是为了normalization。

![1612018994(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612018994(1).png)
![1612019022(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019022(1).png)

计算 (4) 需要知道整个网络的信息以及所有节点的独立模型信息，这对于节点而言是未知的，因为每个节点只知道相邻节点的信息。因此，作者提出下面的迭代形式：
![1612019342(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019342(1).png)

作者证明，无论初始值 $\Theta(0)$ 取何值，上述迭代序列收敛到 (4)。(5) 式可以进一步分解为
![1612019480(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612019480(1).png)

考虑一个同步计算：在每一步，每个节点都和其所有相邻节点进行通信，收集它们当前参数，然后使用它们的参数更新上式。同步更新会导致很大的延迟，因为任何节点都必须等剩余节点更新完后才能进行下一步更新。并且，每一步，所有节点都需要和其邻居节点进行通信，降低了算法的效率。所以作者提出一个异步算法。

## Asynchronous Gossip Algorithm

在异步设置中，每个节点都有一个局部clock ticking at times of rate 1 Poisson process. 由于节点都是独立同分布的，所以相当于在每一步时等概率激活每个节点。

在时间 $t$ 时，每个节点都存有相邻节点的信息。以数学形式表示，考虑矩阵 $\tilde{\Theta}_{i}(t) \in \mathbb{R}^{n \times p}$，第 $i$ 行 $\tilde{\Theta}_{i}^{i}(t) \in \mathbb{R}^{p}$ 为节点 $i$ 在时刻 $t$ 的模型参数，$\tilde{\Theta}_{i}^{j}(t) \in \mathbb{R}^{p} (j \ne i)$ 为节点 $i$ 储存的关于邻居节点 $j$ 的last knowledge. 对于 $j \notin \mathcal{N}_{i} \bigcup \{i\}$，$\forall t > 0$，$\tilde{\Theta}_{i}^{j}(t) = 0$。令$\tilde{\Theta} = [\tilde{\Theta}_{1}^{T}, ...,\tilde{\Theta}_{n}^{T}] \in \mathbb{R}^{n^{2} \times p}$。

如果在时间 $t$ 时，节点 $i$ wakes up，执行如下步骤：

- communication: 节点 $i$ 随机选择一个邻居节点 $j \in \mathcal{N}_{i}$，(先验概率 $\pi_{i}^{j}$)，节点 $i$ 和节点 $j$ 同时更新它们的参数：
$$
\tilde{\Theta}_{i}^{j}(t+1) = \tilde{\Theta}_{j}^{j}(t) \qquad \tilde{\Theta}_{j}^{i}(t+1) = \tilde{\Theta}_{i}^{i}(t),
$$

- update: 基于当前信息，节点 $i$ 和节点 $j$ 更新自己的模型参数：
  $$
  \tilde{\Theta}_{l}^{l}(t+1) = (\alpha +\bar{\alpha}c_{l})^{-1}(\alpha \sum_{k \in \mathcal{N}_{l}} \frac{W_{lk}}{D_{ll}}\tilde{\Theta}_{l}^{k}(t+1) + \bar{\alpha}c_{l}\theta^{sol}_{l}) \quad(l \in \{i,j\}).
  $$

  网络中的其他变量保持不变。作者提出的算法属于 gossip algorithms，每个节点每次最多只和一个邻居节点通信。

  作者证明，上述算法可以收敛到使每个节点具有最优参数。

    ![1612340701](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612340701.jpg)

# Collaborative Learning

上述算法先在局部节点上进行学习，然后在进行网络通信。在这部分，作者提出了一个使节点可以同时进行基于局部数据和邻居节点信息更新模型参数的算法。相较于前面的算法，该算法通信成本较高，但是估计精度高于前者。

## Problem Formulation 

 优化目标：
![1612341031(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341031(1).png)

注意到，这里的置信度通过 $\mathcal{L}_{i}$ 体现，因为 $\mathcal{L}_{i}$ 为局部节点 $i$ 上所有观测的损失函数和。

一般情况下，上述问题没有解析解，作者提出一个分散式迭代算法进行求解。

## Asynchronous Gossip Algorithm

作者基于ADMM提出了一个异步分散式算法。本文的目的不是寻找一个consensus 解，因为我们的目标是为了学习到每个节点的personalized model. 作者通过将问题 (7) 进行变换为一个部分consensus问题，使用ADMMD进行求解。

令 $\Theta_{i} \in \mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}$ 为变量 $\theta_{j} \in \mathbb{R}^{p}(j \in \mathcal{N_{i}} \bigcup \{i\})$ 的集合。定义 $\theta_{j}$ 为 $\Theta_{i}^{j}$。优化问题(7)重新写为：

![1612341938(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612341938(1).png)

在这个目标函数中，所有的节点相互依赖，因为它们共享一个优化变量 $\in \Theta$。为了使用ADMM，需要将各个节点的优化变量独立，对于每个节点 $i$，定义一个local copy $\tilde{\Theta}_{i} \in \mathbb{R}^{(|\mathcal{N_{i}}|+1)\times p}$，添加等式约束：$\tilde{\Theta}_{i}^{i} = \tilde{\Theta}_{j}^{i}$，对于所有的 $i \in [n], j \in \mathcal{N}_{i}$。

![1612342502(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342502(1).png)

增广拉格朗日乘子：

![1612342580(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342580(1).png)

算法如下，假设时刻 $t$ 时节点 $i$ wakes up，选取邻居节点 $j \in \mathcal{N}_{i}$，定义 $e = (i,j)$，

![1612342728(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342728(1).png)

![1612342765(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612342765(1).png)

# Experiments

## Collaborative Linear Classification

考虑100个节点，每个节点的目标是建立一个线性分类模型 in $\mathbb{R}^{p}$。为了方便可视化，每个节点的真实参数位于2维子空间：将其参数看作是 $\mathbb{R}^{p}$ 空间中的向量，前两项从正态分布中随机产生，剩余项为0。两个节点 $i$ 和节点 $j$ 的相似度通过参数距离的高斯核定义，定义 $\phi_{ij}$ 为两个真实参数在单位圆上投影的夹角，$W_{ij} = \exp(\cos \phi_{ij} - 1)/\sigma$，$\sigma = 0.1$。权重为负值的将被忽略。每个节点具有随机的训练样本，样本的标签为二元标签，由线性分类模型产生。以概率0.05随机使标签反转，以产生噪音数据。每个节点的损失函数为hinge损失：$l(\theta;(x_{i}, y_{i})) = \max(0, 1-y_{i}\theta^{T}x_{i})$。作者评估了模型在100个测试样本上的预测精度。

![1612343843(1)](https://cdn.jsdelivr.net/gh/sugar-xue/BlogImages@main/Picbed1612343843(1).png) 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/FL/" rel="tag">FL</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-GD"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/01/27/GD/"
    >Gradient Descent, Stochastic Gradient Descent, Variance Reduction</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2021/01/27/GD/" class="article-date">
  <time datetime="2021-01-27T01:53:22.728Z" itemprop="datePublished">2021-01-27</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/paper/">paper</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="SVRG2013-Accelerating-Stochastic-Gradient-Descent-using-Predictive-Variance-Reduction"><a href="#SVRG2013-Accelerating-Stochastic-Gradient-Descent-using-Predictive-Variance-Reduction" class="headerlink" title="[SVRG2013] Accelerating Stochastic Gradient Descent using Predictive Variance Reduction"></a>[SVRG2013] Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>考虑如下优化问题 </p>
<script type="math/tex; mode=display">
\min P(\omega) = \frac{1}{n}\psi_{i}(\omega)</script><ul>
<li>如果采用平方损失，最小二乘回归；</li>
<li>考虑正则项，令 $\psi<em>{i}(\omega) = \ln(1 + \exp(-\omega^{T}x</em>{i}y<em>{i})) + 0.5\lambda\omega^{T}\omega, y</em>{i} \in {-1,1}$，regularized logistic regression。</li>
</ul>
<p>梯度下降算法更新过程：</p>
<script type="math/tex; mode=display">
\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla P(\omega^{(t-1)}) = \omega^{(t-1)} - \frac{\eta_{t}}{n}\sum_{i=1}^{n}\nabla \psi_{i}(\omega^{(t-1)})</script><p>但是，在每一步，GD都需要计算 $n$ 个一阶偏导，计算量大。所以，一个改进就是随机梯度下降SGD：在每一步迭代时，随机从 ${1,…,n}$ 中抽取 $i_{t}$，然后</p>
<script type="math/tex; mode=display">
\omega^{(t)} = \omega^{(t-1)} - \eta_{t}\nabla \psi_{i_{t}}(\omega^{(t-1)})</script><p>期望 $E[\omega^{(t)}|\omega^{(t-1)}]$ 同梯度更新的结果一致。 SGD的更一般表达形式为</p>
<script type="math/tex; mode=display">
\omega^{(t)} = \omega^{(t-1)} - \eta_{t}g_{t}(\omega^{(t-1)},  \xi_{t})</script><p>$\xi<em>{t}$ 为一个依赖于 $\omega^{(t-1)}$ 的随机变量， 并且期望 $E[g</em>{t}(\omega^{(t-1)},  \xi_{t})|\omega^{(t-1)}] = \nabla P(\omega^{(t-1)})$。 </p>
<p>SGD的优势就是每步迭代只需要计算一个梯度，因此计算成本是GD的 $\frac{1}{n}$。但是，SGD的一个缺点就是随机性引入了方差：虽然 $g<em>{t}(\omega^{(t-1)},  \xi</em>{t})$ 的期望等于梯度 $\nabla P(\omega^{(t-1)})$，但是每个$g<em>{t}(\omega^{(t-1)},  \xi</em>{t})$ 是不同的。方差的出现导致收敛速度变慢。 对于SGD，由于随机取样带来的方差，一般都会要求其步长 $\eta_{t} = O(1/t)$，从而得到一个 sub-linear 收敛率 $O(1/t)$。</p>
<ul>
<li>GD: 每步迭代计算慢，收敛快。</li>
<li>SGD：每步迭代计算快，收敛慢。</li>
</ul>
<p>为了改进SGD，一些学者开始设计算法以减少方差，从而可以使用较大的步长 $\eta_{t}$。有一些算法被提出来，比如：SAG(stochastic average gradient)，SDCA。但是这两个算法需要存储所有的梯度，一些情况下不太实际。因此作者提出了一个新的算法，该算法不需要存储所有的梯度信息，并且有较快的收敛速度，可以应用于非凸优化问题。</p>
<h2 id="Stochastic-Variance-Reduced-Gradient-SVRG"><a href="#Stochastic-Variance-Reduced-Gradient-SVRG" class="headerlink" title="Stochastic Variance Reduced Gradient (SVRG)"></a>Stochastic Variance Reduced Gradient (SVRG)</h2><p>为了保证收敛，SGD的步长必须衰减到0，从而导致收敛率变慢。需要较小步长的原因就是SGD的方差。作者提出一个解决方案。每进行 $m$ 次SGD迭代后，记录当前参数 $\tilde{\omega}$ 以及平均梯度：</p>
<script type="math/tex; mode=display">
\tilde{\mu} = \nabla P(\tilde{\omega}) = \frac{1}{n}\sum_{i=1}^{n} \nabla \psi_{i}(\tilde{\omega}).</script><p>然后接下来的更新为：</p>
<script type="math/tex; mode=display">
\omega^{(t)} = \omega^{(t-1)} - \eta_{t}(\nabla \psi_{i_{t}}(\omega^{(t-1)}) - \nabla \psi_{i_{t}}(\tilde{\omega}) + \tilde{\mu})</script><p>注意到：</p>
<script type="math/tex; mode=display">
E[\omega^{(t)}|\omega^{(t-1)}] = \omega^{(t-1)} - \eta_{t}\nabla P(\omega^{(t-1)})</script><p>算法如下：</p>
<p>  <img src="/2021/01/27/GD/SVRG.png" alt=" "></p>
<p>更新步骤中梯度的方差是减小的。当 $\tilde{\omega}$ 和 $\omega^{(t)}$ 收敛到最优参数 $\omega<em>{*}$，$\tilde{\mu} \to 0$， $\nabla\psi</em>{i}(\tilde{\omega}) \to \nabla \psi<em>{i}(\omega</em>{*})$，有</p>
<script type="math/tex; mode=display">
\nabla\psi_{i}(\omega^{(t-1)}) - \nabla\psi_{i}(\tilde{\omega
}) + \tilde{\mu} \to \nabla\psi_{i}(\omega^{(t-1)}) - \nabla\psi_{i}(\omega_{*}) \to 0</script><p>SVRG的学习率不需要衰减，因此能有较快的收敛速度。作者提到，参数 $m$ 应该 $O(n)$， 比如对于凸问题：$m = 2n$，非凸问题：$m = 2n$。</p>
<h1 id="SAGA2015-SAGA-A-Fast-Incremental-Gradient-Method-With-Support-for-Non-Strongly-Convex-Composite-Objectives"><a href="#SAGA2015-SAGA-A-Fast-Incremental-Gradient-Method-With-Support-for-Non-Strongly-Convex-Composite-Objectives" class="headerlink" title="[SAGA2015] SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives"></a>[SAGA2015] SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives</h1><p>考虑最小化函数：</p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{n}\sum_{i=1}^{n}f_{i}(x)</script><p>作者提出了一个叫做SAGA的算法，在目标函数为强凸函数时，SAGA的收敛速度优于SAG和SVRG。算法如下：</p>
<p>  <img src="/2021/01/27/GD/SAGA.png" alt=" "></p>
<p>  本文给出了variance reduction算法的一个解释：<br>   <img src="/2021/01/27/GD/SAGA1.png" alt=" "></p>
<p> 几种算法比较：<br>   <img src="/2021/01/27/GD/SAGA2.png" alt=" "></p>
<h1 id="Variance-Reduced-Stochastic-Gradient-Descent-with-Neighbors-2015"><a href="#Variance-Reduced-Stochastic-Gradient-Descent-with-Neighbors-2015" class="headerlink" title="[Variance Reduced Stochastic Gradient Descent with Neighbors 2015]"></a>[Variance Reduced Stochastic Gradient Descent with Neighbors 2015]</h1><h1 id="Katyusha-2017-Katyusha-The-First-Direct-Acceleration-of-Stochastic-Gradient-Methods"><a href="#Katyusha-2017-Katyusha-The-First-Direct-Acceleration-of-Stochastic-Gradient-Methods" class="headerlink" title="[Katyusha 2017] Katyusha: The First Direct Acceleration of Stochastic Gradient Methods"></a>[Katyusha 2017] Katyusha: The First Direct Acceleration of Stochastic Gradient Methods</h1><p>Nesterov’s momentum 通常用于加速梯度下降算法，但是，对于随机梯度下降，Nesterov’s momentum 可能无法对算法进行加速，即使优化目标为凸函数。因此，针对SGD，作者提出 Katyusha 算法，借助于动量Katyusha momentum实现加速SGD。</p>
<p>考虑如下优化问题：</p>
<script type="math/tex; mode=display">
\min_{x \in \mathbb{R}^{d}} \{F(x) = f(x) + \psi(x) = \frac{1}{n}\sum_{i=1}^{n} f_{i}(x) + \psi(x)\}</script><p>其中 $f(x) = \frac{1}{n}\sum<em>{i=1}^{n} f</em>{i}(x)$ 为凸函数，并且是 $n$ 个凸函数的有限平均。$\psi(x)$ 为凸函数，可为近端函数。大多数假设 $\psi(x)$ 为 $\sigma$-strongly，并且 $f_{i}(x)$ L-smooth。</p>
<p>作者提出一个可以求解上述优化问题的加速随机梯度下降算法-Katyusha：</p>
<p>   <img src="/2021/01/27/GD/ka.png" alt=" "></p>
<p>其中，$\tilde{x}$ 为snapshot point，每经过 $n$ 次迭代更新一次。$\tilde{\nabla}<em>{k+1}$ 为variance reduction 中的梯度形式。$\tau</em>{1}$，$\tau<em>{2} \in[0,1]$ 为两个动量参数，$\alpha = \frac{1}{3\tau</em>{1}L}$。</p>
<h2 id="Our-New-Technique-–-Katyusha-Momentum"><a href="#Our-New-Technique-–-Katyusha-Momentum" class="headerlink" title="Our New Technique – Katyusha Momentum"></a>Our New Technique – Katyusha Momentum</h2><p>最novel的部分是 $x<em>{k+1}$ 的更新步骤，是 $y</em>{k}$, $z<em>{k}$ 以及 $\tilde{x}$ 的凸组合。理论建议参数 $\tau</em>{2} = 0.5$，$\tau_{1} = \min{\sqrt{n\sigma/L}, 0.5}$。</p>
<p>对于传统的加速梯度下降算法，$x<em>{k+1}$ 仅仅是 $y</em>{k}$ 和 $z<em>{k}$ 的凸组合，$z</em>{k}$ 起到了一个“动量”的作用，即将历史加权的梯度信息添加到 $y<em>{k+1}$ 上。比如假设 $\tau</em>{2} = 0, \tau<em>{1} = \tau$, $x</em>{0} = y<em>{0} = z</em>{0}$，我们可以得到：</p>
<p>   <img src="/2021/01/27/GD/ka1.png" alt=" "></p>
<p>由于 $\alpha$ 通常大于 $1/3L$，上述递推过程意味着随着迭代进行，梯度 $\tilde{\nabla}<em>{t}$ 的贡献越高。比如，$\tilde{\nabla}</em>{1}$ 的权重不断增大 ($\frac{1}{3L} &lt; ((1-\tau)\frac{1}{3L} + \tau\alpha) &lt; ((1 - \tau)^{2}\frac{1}{3L} + (1 - (1 - \tau)^{2})\alpha)$)。这就是一阶加速方法的核心思想。</p>
<p>在Katyusha算法中，作者认为 $\tilde{x}$ 同等重要，它能保证 $x<em>{k+1}$ 不要太远离 $\tilde{x}$。$\tilde{x}$ 的添加可以看作是一个 “negative momentum”，使 $x</em>{k+1}$ back to $\tilde{x}$，抵消一部分前面迭代时的 positive momentum”。</p>
<p>当 $\tau<em>{1} = \tau</em>{2} = 0.5$ 时，Katyusha同SVRG几乎一致。</p>
<h1 id="L-SVRG-L-Katyusha-2019-Don’t-Jump-Through-Hoops-and-Remove-Those-Loops-SVRG-and-Katyusha-are-BetterWithout-the-Outer-Loop"><a href="#L-SVRG-L-Katyusha-2019-Don’t-Jump-Through-Hoops-and-Remove-Those-Loops-SVRG-and-Katyusha-are-BetterWithout-the-Outer-Loop" class="headerlink" title="[L-SVRG, L-Katyusha 2019] Don’t Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are BetterWithout the Outer Loop"></a>[L-SVRG, L-Katyusha 2019] Don’t Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are BetterWithout the Outer Loop</h1><p>SVRG和Katyusha算法的共同关键结构就是两者都包含一个外层循环 (outer loop)。最初先在outer loop上使用所有样本计算梯度，然后计算出来的结果再用于内层循环 (inner loop)，结合新的随机梯度信息，构造variance-reduced 梯度估计量。作者指出，由于SVRG和Katyusha算法都包括一个outer loop，所以存在一些问题，比如：算法很难分析；人们需要决定内部循环的次数。对于SVRG，理论上内部循环的最优次数取决于 $L$和 $\mu$，但是 $\mu$ 通常未知。由于这些问题存在，人们只能选择次优的inner loop size，通常设置内部循环次数为 $O(n)$ 或者 $n$。</p>
<p>在这篇论文中，作者将外层循环 (outer loop) 丢弃，在每次迭代时采用掷硬币技巧决定是否计算梯度，从而解决了上述问题。作者证明，新提出的算法和原始两个算法具有同样的理论性质。</p>
<p>   <img src="/2021/01/27/GD/loopless1.jpg" alt=" "><br>   <img src="/2021/01/27/GD/loopless1.jpg" alt=" "></p>
<h1 id="L-SVRG-L-Katyusha-2019-L-SVRG-and-L-Katyusha-with-Arbitrary-Sampling"><a href="#L-SVRG-L-Katyusha-2019-L-SVRG-and-L-Katyusha-with-Arbitrary-Sampling" class="headerlink" title="[L-SVRG, L-Katyusha 2019] L-SVRG and L-Katyusha with Arbitrary Sampling"></a>[L-SVRG, L-Katyusha 2019] L-SVRG and L-Katyusha with Arbitrary Sampling</h1> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GD/" rel="tag">GD</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-AL2SGD"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/01/26/AL2SGD/"
    >Lower Bounds and Optimal Algorithms for Personalized Federated Learning</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2021/01/26/AL2SGD/" class="article-date">
  <time datetime="2021-01-26T14:12:22.000Z" itemprop="datePublished">2021-01-26</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/paper/">paper</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <!-- TOC -->
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#contributions">Contributions</a></li>
<li><a href="#lower-complexity-bounds">Lower complexity bounds</a><ul>
<li><a href="#lower-complexity-bounds-on-the-communication">Lower complexity bounds on the communication</a></li>
<li><a href="#lower-complexity-bounds-on-the-local-computation">Lower complexity bounds on the local computation</a></li>
</ul>
</li>
<li><a href="#优化算法">优化算法</a><ul>
<li><a href="#accelerated-proximal-gradient-descent-apgd-for-federated-learning">Accelerated Proximal Gradient Descent (APGD) for Federated Learning</a></li>
<li><a href="#beyond-proximal-oracle-inexact-apgd-iapgd">Beyond proximal oracle: Inexact APGD (IAPGD)</a></li>
<li><a href="#accelerated-l2sgd">Accelerated L2SGD+</a></li>
</ul>
</li>
<li><a href="#experiments">Experiments</a></li>
</ul>
<!-- /TOC -->
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者在前文考虑了一个新的优化问题：</p>
<script type="math/tex; mode=display">
\min_{\bm{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\bm{x}) := f(\bm{x}) + \lambda \psi(\bm{x})\}</script><script type="math/tex; mode=display">
f(\bm{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{x_{i}}), \quad \psi(\bm{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\bm{x_{i}} - \bm{\bar{x}}\|^{2}</script><p>Remark：该问题的最优解 $\bm{x}^{<em>} = [\bm{x}_{1}^{</em>},…, \bm{x}<em>{n}^{<em>}] \in \mathbb{R}^{nd}$ 可以被表示为 $\bm{x}^{</em>}</em>{i} = \bm{\bar{x}}^{<em>} - \frac{1}{\lambda}\nabla f<em>{i}(\bm{x}</em>{i}^{</em>})$，其中 $\bm{\bar{x}}^{<em>} = \frac{1}{n}\sum<em>{i=1}^{n} \bm{x}</em>{i}^{</em>}$，该形式与MAML相似。</p>
<h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><p>在这篇论文中，作者给出了求解上述优化问题的通信和局部计算复杂度（迭代次数）的最低界限，并且给出了几种能够达到最低界限的算法。</p>
<ul>
<li>lower bound on the communication complexity. 作者证明对于任意一个满足一定假设条件的算法，会有一个L-smooth, $\mu$-strongly convex 局部目标函数 $f_{i}$ 至少需要通信 $O(\sqrt{\frac{\min{L,  \lambda}}{\mu}}\log\frac{1}{\epsilon})$ 轮才能得到最优解 $\epsilon$邻域内的解。</li>
<li><p>lower complexity bound on the number of local oracle calls. 作者证明对于局部近端梯度下降，至少需要迭代$O(\sqrt{\frac{\min{L,  \lambda}}{\mu}}\log\frac{1}{\epsilon})$ 次；对于局部梯度下降，至少需要进行 $O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})$ 次迭代；若每个目标函数为 $m$ 个有限和形式（$\tilde{L}$-smooth)，至少需要 $O((m + \sqrt{\frac{m\tilde{L}}{\mu}})\log\frac{1}{\epsilon})$ 次。</p>
</li>
<li><p>作者讨论了不同的用于求解上述优化问题的算法，这些算法在不同设定下可以达到最优通信复杂度和最优局部梯度复杂度。。首先是加速近端梯度下降算法(APGD)，作者考虑两种不同的应用方式，第一种是：函数 $f$ 采用梯度下降，$\lambda \psi$ 采用近端梯度下降，第二种是反过来。对于第一种情况，当 $L \leq \lambda$ 时，我们可以实现最优通信复杂度和局部梯度复杂度 $O(\sqrt{\frac{L}{\mu}}\log\frac{1}{\epsilon})$；对于第二种情况，当 $L \geq \lambda$ 时，我们可以得到最优通信复杂度和局部近端复杂度 $O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})$。 受一篇论文启发，作者提到局部近端可以由局部加速梯度下降 (Local AGD) 近似 (inexactly) 得到，当目标函数为有限和形式，还可以采用 Katyusha算法近似得到。Local AGD 可以得到 $O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})$ 的通信复杂度，以及 $\tilde{O}(\sqrt{\frac{L+\lambda}{\mu}})$ 的局部梯度复杂度，当 $L \geq \lambda$（取决于对数因子） 时，两者都能达到最优。同样，当局部采用 Katyusha 时，我们可以得到通信复杂度 $O(\sqrt{\frac{\lambda}{\mu}}\log\frac{1}{\epsilon})$ 和局部梯度复杂度 $\tilde{O}(m\sqrt{\frac{\lambda}{\mu}} + \sqrt{m \frac{\tilde{L}}{\mu}})$，前者当 $L \geq \Lambda$ 时能达到最优，后者当 $m\lambda \leq \tilde{L}$（取决于对数因子） 时达到最优。</p>
</li>
<li><p>作者提出了加速的L2SGD+算法-AL2SGD+，该算法可以实现最优通信复杂  度 $O(\sqrt{\frac{\min{\tilde{L},  \lambda}}{\mu}}\log\frac{1}{\epsilon})$，以及局部梯度复杂度 $O((m + \sqrt{\frac{m(\tilde{L} + \lambda)}{\mu}})\log\frac{1}{\epsilon})$，当 $\lambda \leq \tilde{L}$ 时最优。但是，两者无法同时实现最优。</p>
<p><img src="/2021/01/26/AL2SGD/t1.jpg" alt=" "></p>
</li>
</ul>
<h1 id="Lower-complexity-bounds"><a href="#Lower-complexity-bounds" class="headerlink" title="Lower complexity bounds"></a>Lower complexity bounds</h1><h2 id="Lower-complexity-bounds-on-the-communication"><a href="#Lower-complexity-bounds-on-the-communication" class="headerlink" title="Lower complexity bounds on the communication"></a>Lower complexity bounds on the communication</h2><p>  <img src="/2021/01/26/AL2SGD/3.1.png" alt=" "><br>  <img src="/2021/01/26/AL2SGD/3.11.png" alt=" "></p>
<h2 id="Lower-complexity-bounds-on-the-local-computation"><a href="#Lower-complexity-bounds-on-the-local-computation" class="headerlink" title="Lower complexity bounds on the local computation"></a>Lower complexity bounds on the local computation</h2><p>  <img src="/2021/01/26/AL2SGD/3.2.png" alt=" "></p>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="Accelerated-Proximal-Gradient-Descent-APGD-for-Federated-Learning"><a href="#Accelerated-Proximal-Gradient-Descent-APGD-for-Federated-Learning" class="headerlink" title="Accelerated Proximal Gradient Descent (APGD) for Federated Learning"></a>Accelerated Proximal Gradient Descent (APGD) for Federated Learning</h2><p>首先介绍非加速版本的近端梯度下降算法(PGD):<br>  <img src="/2021/01/26/AL2SGD/1.png" alt=" "></p>
<p>根据另一篇论文，有两种不同的方式可以将梯度下降算法应用到上述优化问题上。最直接的方式是令 $h = f$，$\phi = \lambda\psi$，那么可以得到如下更新步骤：<br>  <img src="/2021/01/26/AL2SGD/2.png" alt=" "></p>
<p>另一种方式是令 $h(\bm{x}) = \lambda \phi(\bm{x}) + \frac{\mu}{2n}|\bm{x}|^{2}$， $\phi(\bm{x}) = f(\bm{x}) - \frac{\mu}{2n}|\bm{x}|^{2}$。由此得到的更新过程如下：<br>  <img src="/2021/01/26/AL2SGD/3.png" alt=" "></p>
<p>同FedProx算法一致。</p>
<p>由于上述两种情况下，每次迭代都需要进行一轮通信，相应的通信复杂度次优。但是可以结合动量算法，程序(6) 可以结合Nesterov’s momentum，能够得到最优通信复杂度，以及最优局部近端复杂度（当 $\lambda \leq L)$，该算法定义为APGD1，具体如下：</p>
<p>  <img src="/2021/01/26/AL2SGD/a2.png" alt=" "></p>
<p>将更新过程(5)和动量结合，可以得到最优通信复杂度以及最优局部近端复杂度（当 $\lambda \geq L）$。将该算法定义为APGD2，具体如下：</p>
<p>  <img src="/2021/01/26/AL2SGD/a3.png" alt=" "></p>
<h2 id="Beyond-proximal-oracle-Inexact-APGD-IAPGD"><a href="#Beyond-proximal-oracle-Inexact-APGD-IAPGD" class="headerlink" title="Beyond proximal oracle: Inexact APGD (IAPGD)"></a>Beyond proximal oracle: Inexact APGD (IAPGD)</h2><p>在多数情况下，如果采用局部近端操作，每一步迭代时都需要得到子问题的精确解，这是不实际的。因此，作者提出了一个针对(6) 的加速非精确的算法，每个节点只需要进行局部梯度运算（AGD, Katyusha)：</p>
<p>  <img src="/2021/01/26/AL2SGD/a1.png" alt=" "></p>
<h2 id="Accelerated-L2SGD"><a href="#Accelerated-L2SGD" class="headerlink" title="Accelerated L2SGD+"></a>Accelerated L2SGD+</h2><p>作者给出L2SGD+算法的一个加速版本-AL2SGD+。作者指出AL2SGD+算法不过是L-Katyusha 算法与非均匀抽样的结合。</p>
<p>  <img src="/2021/01/26/AL2SGD/a4.png" alt=" "></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>在第一个实验中，作者比较了当局部损失为有限和形式时，算法IAPGD+Katyusha、AL2SGD+以及L2SGD+的收敛速度。结果如下图：</p>
<p>  <img src="/2021/01/26/AL2SGD/5.jpg" alt=" "></p>
<p>对于通信轮数，IAPGD+Katyusha和AL2SGD+都显著优于L2SGD+；对于局部计算次数，AL2SGD+表现最优，IAPGD+Katyusha不如L2SGD+。</p>
<p>第二个实验中，作者研究了数据异质性对算法的影响，结果如下图所示。可以看出，数据异质性不影响算法的收敛速度，各个算法的表现同第一个实验相似。</p>
<p>  <img src="/2021/01/26/AL2SGD/6.png" alt=" "></p>
<p>在第三个实验中，作者比较了APGD算法的两种变形：APGD1和APGD2。作者不断改变参数 $\lambda$ 的取值，其余参数保持不变。在理论上，APGD2算法应该不受参数 $\lambda$ 影响，而APGD1 算法的收敛率会随着 $\lambda$ 而增加 ($\sqrt{\lambda}$)。 当 $\lambda \leq L = 1$时，APGD1是最优选择；当 $\lambda &gt; L = 1$ 时，APGD2 应该是最优选择。实验结果如下图所示，结果与理论一致。</p>
<p><img src="/2021/01/26/AL2SGD/7.png" alt=" "></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/FL/" rel="tag">FL</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-L2SGD"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/01/26/L2SGD/"
    >Federated Learning of a Mixture of Global and Local Models</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2021/01/26/L2SGD/" class="article-date">
  <time datetime="2021-01-26T14:12:10.000Z" itemprop="datePublished">2021-01-26</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/paper/">paper</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <!-- TOC -->
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#11-federated-learning">1.1 Federated learning</a></li>
</ul>
</li>
<li><a href="#contributions">Contributions</a></li>
<li><a href="#新的优化问题">新的优化问题</a></li>
<li><a href="#l2gd-loopless-local-gd">L2GD: Loopless Local GD</a><ul>
<li><a href="#收敛理论">收敛理论</a></li>
<li><a href="#收敛率优化">收敛率优化</a></li>
</ul>
</li>
<li><a href="#loopless-local-sgd-with-variance-reduction">Loopless Local SGD with Variance Reduction</a><ul>
<li><a href="#问题设置">问题设置</a></li>
<li><a href="#理论">理论</a></li>
</ul>
</li>
<li><a href="#experiments">Experiments</a></li>
<li><a href="#附录">附录</a><ul>
<li><a href="#experimental-setup-and-further-experiments">Experimental Setup and further experiments</a></li>
<li><a href="#其余算法">其余算法</a></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="1-1-Federated-learning"><a href="#1-1-Federated-learning" class="headerlink" title="1.1 Federated learning"></a>1.1 Federated learning</h2><p>  联邦学习的目标函数：</p>
<script type="math/tex; mode=display">
  \min_{\bm{x} \in \mathbb{R}^{d}} \frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{x})</script><p>其中 $n$表示参与训练的节点个数，$\bm{x} \in \mathbb{R}^{d}$为全模型优化变量。 $f_{i}(\bm{x})$为节点$i$上的损失函数。</p>
<h1 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h1><ul>
<li>提出了新的FL优化形式，尝试学习全局模型和局部模型的混合。</li>
<li>给出了新的优化形式的理论性质。作者证明了最优局部模型以$O(1/ \lambda)$收敛到传统的全局模型；作者证明了在局部模型上得到的损失不高于全局模型上的损失(定理3.1)；作者指出局部模型的最优解等于所有局部模型最优解的平均值减去对应局部模型上损失函数的一阶梯度，这一点和MAML一致。</li>
<li>Loopless LGD：作者提出了一个随机梯度算法 — Loopless Local Gradient Descent (L2GD)（算法1）来解决提出的优化问题。该算法不是一个标准的SGD，它可以看作是一个关于损失函数和惩罚项的不均匀抽样。当抽到损失函数部分时，每个节点执行一次随机梯度下降；当抽到惩罚项时，进行信息聚合。</li>
<li>收敛理论。假设函数 $f_{i}$ 为 $L-smooth$，并且为 $\mu-strong \, convex$，可以得到抽样概率 $p^{*} = \frac{\lambda}{\lambda + L}$，固定期望局部更新次数为 $1 + \frac{L}{\lambda}$，作者证明通信 (communication) 复杂度为（通信次数上界）为 $\frac{2\lambda}{\lambda + L}\frac{L}{\mu}\log\frac{1}{\epsilon}$。当 $\lambda \to 0$时，通信次数非常小；当 $\lambda \to \infty $时，根据新优化问题得到的解收敛到全局模型最优解，并且L2GD 算法的通信上界为 $O(\frac{L}{\mu}\log\frac{1}{\epsilon})$。</li>
<li>推广。部分连接，局部SGD，variance reduction（variance来自三部分：非均匀抽样，部分连接，从节点样本随机抽样）。</li>
<li>可用于异质数据。</li>
<li>经验表现不错。</li>
</ul>
<h1 id="新的优化问题"><a href="#新的优化问题" class="headerlink" title="新的优化问题"></a>新的优化问题</h1><script type="math/tex; mode=display">
\min_{\bm{x_{1},...,x_{n}} \in \mathbb{R}^{d}} \{ F(\bm{x}) := f(\bm{x}) + \lambda \psi(\bm{x})\}</script><script type="math/tex; mode=display">
f(\bm{x}):= \frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{x_{i}}), \quad \psi(\bm{x}):= \frac{1}{2n}\sum_{i=1}^{n}\|\bm{x_{i}} - \bm{\bar{x}}\|^{2}</script><ul>
<li>Local model ($\lambda = 0$)</li>
<li>Mixed model ($\lambda \in (0, \infty)$)</li>
<li>Global model ($\lambda = \infty$)</li>
</ul>
<h1 id="L2GD-Loopless-Local-GD"><a href="#L2GD-Loopless-Local-GD" class="headerlink" title="L2GD: Loopless Local GD"></a>L2GD: Loopless Local GD</h1><p>在这一部分中，作者给出一个算法求解上述优化问题，该算法可以看作是一个非均匀SGD，要么抽取 $\nabla f$，要么抽取 $\nabla \psi$ 估计 $\nabla F$。令 $0 &lt; p &lt; 1$，定义一个随机梯度如下：</p>
<script type="math/tex; mode=display">
G(\bm{x}):= \begin{cases} \frac{\nabla f(\bm{x})}{1-p}, & \text {概率 $1-p$} \\ \frac{\lambda \nabla \psi(\bm{x})}{p}, & \text{概率 $p$ } \end{cases}</script><p>显然，$G(\bm{x})$为 $\nabla F(\bm{x})$的无偏估计量。<br>每步的更新为:</p>
<script type="math/tex; mode=display">
\bm{x}^{k+1} = \bm{x}^{k} - \alpha G(\bm{x}).</script><p><img src="/2021/01/26/L2SGD/l2gd.png" alt="Algorithm 1"></p>
<p>$\textbf{Lemma 4.2}$  经过 $k$ 步迭代后，期望的通信次数为 $p(1-p)k$。</p>
<h2 id="收敛理论"><a href="#收敛理论" class="headerlink" title="收敛理论"></a>收敛理论</h2><p>作者首先证明梯度估计量$G(\bm{x})$的期望具有光滑性质，然后证明了算法L2GD的收敛性质。（$\bm{x(\lambda)}$为最优解，定理4.4 表明，L2GD算法只能收敛到最优解邻域。）<br><img src="/2021/01/26/L2SGD/4.3.png" alt=" "></p>
<h2 id="收敛率优化"><a href="#收敛率优化" class="headerlink" title="收敛率优化"></a>收敛率优化</h2><p>作者给出最优抽样概率 $p^{*} = \frac{\lambda}{L + \lambda}$，步长 $\alpha$ 要满足 $\frac{\alpha\lambda}{np} \leq \frac{1}{2}$.<br><img src="/2021/01/26/L2SGD/4.4.png" alt=" "></p>
<h1 id="Loopless-Local-SGD-with-Variance-Reduction"><a href="#Loopless-Local-SGD-with-Variance-Reduction" class="headerlink" title="Loopless Local SGD with Variance Reduction"></a>Loopless Local SGD with Variance Reduction</h1><p>L2GD算法仅线性收敛到最优解的邻域，无法收敛到最优解。假设每个子目标函数具有有限和形式，作者提出了一个算法L2SGD+，在每个节点上进行随机梯度下降，并且具有线性收敛速度。L2SGD是一个具有variance reduction 的局部SGD算法，关于SGD的variance reduction，见另一篇博客：SGD with variance reduction.</p>
<h2 id="问题设置"><a href="#问题设置" class="headerlink" title="问题设置"></a>问题设置</h2><p>假设 $f_{i}$ 具有有限和结构：</p>
<script type="math/tex; mode=display">
f_{i} = \frac{1}{m}\sum_{j=1}^{m}f_{i,j}(\bm{x}_{i})</script><p>那么目标函数变为：</p>
<script type="math/tex; mode=display">
F(\bm{x}) = \frac{1}{n}\sum_{i=1}^{n}(\frac{1}{m}\sum_{i=1}^{m}f_{i,j}(\bm{x}_{i})) + \lambda\frac{1}{2n}\sum_{i=1}^{n}\|\bm{x}_{i} - \bm{\bar{x}}\|^{2}</script><p><img src="/2021/01/26/L2SGD/l2sgd+.png" alt=" "></p>
<p>L2SGD算法仅在两次抽样不同时才会发生通信，经过 $k$ 次迭代后，需要进行 $p(1-p)k$ 次聚合平均。但是，L2SGD算法还需要通信控制变量 $\bm{J<em>{i}I,  \Psi</em>{i}}$，因此通信次数变为原来的3倍。在附录中，作者给出了一个高效的L2SGD+，不需要通信控制变量。</p>
<h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>作者给出了L2SGD算法的理论性质，并且给出最优抽样概率 $p^{*}$。</p>
<p><img src="/2021/01/26/L2SGD/5.1.jpg" alt=" "><br><img src="/2021/01/26/L2SGD/5.2.png" alt=" "><br><img src="/2021/01/26/L2SGD/5.3.png" alt=" "></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>作者考虑Logistic回归问题，数据为LibSVM data(Chank &amp; Lin, 2011)。数据首先进行normalized，以使得 $f_{ij}$ 为1-smooth。步长根据定理5.2确定。每个数据集被划分为不同个数的节点，具体参数设置如下：<br><img src="/2021/01/26/L2SGD/table1.png" alt=" "></p>
<p>作者考虑三种算法：L2SGD+, L2SGD(L2GD with local SGD), L2SGD2(L2GD with local subsampling and control variates constructed for $\Psi$)。根据理论分析，L2SGD+线性收敛到最优解，而L2SGD和L2SGD2收敛到最优解邻域。</p>
<p>作者考虑了两种数据分割方式。对于homogeneous data, 首先将观测样本随机打乱，然后按照打乱后的数据划分到不同节点上；对于heterogeneous data, 首先根据观测样本的标签将样本排序，然后将排序后的数据依次划分到不同节点上 (the worst-case heterogeneity)。</p>
<p><img src="/2021/01/26/L2SGD/figure3.png" alt=" "></p>
<p>结果表明</p>
<ul>
<li>L2SGD+ (Full variance reduction)可以收敛到最优解，而 L2SGD(without variance reduction)和 L2SGD2(with partial variance<br>reduction) 只收敛到最优解邻域。</li>
<li>进行variance reduction是非常有必要的。它可以保证较快的全局收敛。</li>
<li>数据异质性对算法收敛性没有影响。</li>
</ul>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="Experimental-Setup-and-further-experiments"><a href="#Experimental-Setup-and-further-experiments" class="headerlink" title="Experimental Setup and further experiments"></a>Experimental Setup and further experiments</h2><ul>
<li>参数 $p$ 如何影响算法L2SGD+的收敛速度</li>
<li>参数 $\lambda$ 如何影响算法L2SGD+的收敛速度</li>
</ul>
<h2 id="其余算法"><a href="#其余算法" class="headerlink" title="其余算法"></a>其余算法</h2><ul>
<li><p>Local GD with variance reduction</p>
<p>当每个节点采用梯度下降算法，且考虑variance reduction时，</p>
<p><img src="/2021/01/26/L2SGD/b1.png" alt=" "><br><img src="/2021/01/26/L2SGD/a3.png" alt=" "></p>
</li>
<li><p>Efficient implementation of L2SGD+<br>考虑到L2SGD+需要通信控制变量，增加了通信次数。作者给出了一个高效的版本，不需要通信控制变量，$k$次迭代只需要通信 $p(1-p)k$次。</p>
<p><img src="/2021/01/26/L2SGD/a4.png" alt=" "></p>
</li>
<li><p>Local SGD with variance reduction – general method<br>在这部分中，作者给出了一个使用性更广的版本。每个节点上目标函数可以包含一个非光滑正则项：</p>
<p> <img src="/2021/01/26/L2SGD/b3.png" alt=" "></p>
<p>另外，该版本算法允许从所有节点中任意抽样，允许节点结构任意（比如节点数据集大小，目标函数光滑程度，每个节点抽样方式任意）。</p>
<p><img src="/2021/01/26/L2SGD/a5.png" alt=" "></p>
</li>
</ul>
<ul>
<li><p>Local stochastic algorithms</p>
<p>在这部分中，作者给出两个简单算法，不考虑variance reduction的Local SGD(算法6)以及只考虑部分variance reduction的Local SGD (算法7)。</p>
<p><img src="/2021/01/26/L2SGD/a6.png" alt=" "></p>
<p><img src="/2021/01/26/L2SGD/a7.png" alt=" "></p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/FL/" rel="tag">FL</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-hello-world"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/01/22/hello-world/"
    >Hello World</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2021/01/22/hello-world/" class="article-date">
  <time datetime="2021-01-22T12:49:59.736Z" itemprop="datePublished">2021-01-22</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> Xue Yu
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Sugar"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%97%85%E8%A1%8C/">旅行</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2021/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>